import pandas as pd
import shutil
import logging
import xlwt
from pathlib import Path
from datetime import datetime
from typing import List, Generator, Dict, Optional, Tuple, Any
import time
import os
import logger_config  # å°å…¥çµ±ä¸€çš„æ—¥èªŒé…ç½®

# ä½¿ç”¨çµ±ä¸€çš„æ—¥èªŒé…ç½®
logger = logging.getLogger(__name__)

# --- è¨­å®šç®¡ç†å™¨ (å–®ä¸€è¡¨æ ¼æ¨¡å¼) ---
class ConfigManager:
    def __init__(self, base_dir: Path):
        self.settings_dir = base_dir / "settings"
        self.settings_file = self.settings_dir / "supplier_config.xlsx"
        self.settings_dir.mkdir(parents=True, exist_ok=True)
        
        # é è¨­ç¯„ä¾‹ (ä¾ç…§ä½ çš„è¦æ±‚ï¼šä¾›æ‡‰å•†åç¨±åªæ˜¯é¡¯ç¤ºç”¨)
        self.DEFAULT_DATA = [
            {
                "ä¾›æ‡‰å•†åç¨±": "å¯ç¾å¡‘æ–™åˆ¶å“æœ‰é™å…¬å¸", # é€™æ˜¯çµ¦äººçœ‹çš„ ID
                "è²¨å“æ¢ç¢¼": "æ¡å½¢ç ",             # å¿…é ˆå®Œå…¨åŒ¹é…æ”¶æ“šè£¡çš„æ¬„ä½å
                "å…¥è²¨åƒ¹": "å•ä»·",
                "å…¥è²¨é‡": "æ•°é‡",
                "è²¨å“åç¨±": "è´§å“åç§°åŠè§„æ ¼å‹å·",
                "æˆæœ¬ä¹˜ä»¥1.2": "æ˜¯"
            }
        ]

    def load_config(self) -> pd.DataFrame:
        """è®€å– Excel è¨­å®š"""
        if not self.settings_file.exists():
            self._create_default_config()
        
        try:
            df = pd.read_excel(self.settings_file)
            # ç¢ºä¿æ¬„ä½éƒ½æ˜¯å­—ä¸²ï¼Œä¸¦å»é™¤å‰å¾Œç©ºç™½
            df = df.astype(str).apply(lambda x: x.str.strip())
            logger.info("âœ… ä¾›æ‡‰å•†è¨­å®šæª”è®€å–æˆåŠŸ")
            return df
        except Exception as e:
            logger.error(f"âŒ è®€å–è¨­å®šæª”å¤±æ•—: {e}")
            return pd.DataFrame(self.DEFAULT_DATA)

    def _create_default_config(self):
        """ç”¢ç”Ÿçµ¦å“¡å·¥å¡«å¯«çš„ Excel"""
        logger.info(f"âš ï¸ å»ºç«‹é è¨­è¨­å®šæª”: {self.settings_file}")
        df = pd.DataFrame(self.DEFAULT_DATA)
        cols = ["ä¾›æ‡‰å•†åç¨±", "è²¨å“æ¢ç¢¼", "å…¥è²¨åƒ¹", "å…¥è²¨é‡", "è²¨å“åç¨±", "æˆæœ¬ä¹˜ä»¥1.2"]
        df = df[cols]
        df.to_excel(self.settings_file, index=False)


# --- Mapping ç®¡ç†å™¨ ---
class MappingManager:
    def __init__(self, base_dir: Path):
        """
        ç®¡ç†æ¢ç¢¼åˆ°è²¨å“ç·¨è™Ÿçš„æ˜ å°„é—œä¿‚
        
        Args:
            base_dir: å·¥ä½œç›®éŒ„è·¯å¾‘
        """
        self.settings_dir = base_dir / "settings"
        self.mapping_file = self.settings_dir / "barcode_mapping.xlsx"
        self.settings_dir.mkdir(parents=True, exist_ok=True)
        self.mapping_df = self.load_mapping()
    
    def load_mapping(self) -> pd.DataFrame:
        """è®€å–ç¾æœ‰çš„ mapping"""
        if not self.mapping_file.exists():
            # å»ºç«‹ç©ºçš„ mapping æª”æ¡ˆ
            df = pd.DataFrame(columns=['è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'è²¨å“ç·¨è™Ÿ', 'ä¾›æ‡‰å•†åç¨±', 'å»ºç«‹æ—¥æœŸ'])
            df.to_excel(self.mapping_file, index=False)
            logger.info(f"ğŸ“ å»ºç«‹æ–°çš„ mapping æª”æ¡ˆ: {self.mapping_file}")
            return df
        
        try:
            df = pd.read_excel(self.mapping_file)
            # ç¢ºä¿æ¬„ä½éƒ½æ˜¯å­—ä¸²ï¼Œä¸¦å»é™¤å‰å¾Œç©ºç™½
            df = df.astype(str).apply(lambda x: x.str.strip())
            logger.info(f"âœ… å·²è¼‰å…¥ mapping è¨˜éŒ„: {len(df)} ç­†")
            return df
        except Exception as e:
            logger.error(f"âŒ è®€å– mapping æª”æ¡ˆå¤±æ•—: {e}")
            return pd.DataFrame(columns=['è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'è²¨å“ç·¨è™Ÿ', 'ä¾›æ‡‰å•†åç¨±', 'å»ºç«‹æ—¥æœŸ'])
    
    def find_mapping(self, barcode: str, product_name: str) -> Optional[str]:
        """
        æŸ¥æ‰¾æ˜¯å¦æœ‰å°æ‡‰çš„ mapping
        
        Args:
            barcode: è²¨å“æ¢ç¢¼
            product_name: è²¨å“åç¨±
        
        Returns:
            å°æ‡‰çš„è²¨å“ç·¨è™Ÿï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        if self.mapping_df.empty:
            return None
        
        # æ¸…æ´—æ¢ç¢¼æ ¼å¼ï¼ˆèˆ‡é©—è­‰é‚è¼¯ä¸€è‡´ï¼‰
        barcode_clean = pd.Series([str(barcode)]).str.strip().str.replace(r'\.0+$', '', regex=True).iloc[0]
        product_name_clean = str(product_name).strip()
        
        # æŸ¥æ‰¾åŒ¹é…çš„è¨˜éŒ„
        mask = (
            (self.mapping_df['è²¨å“æ¢ç¢¼'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True) == barcode_clean) &
            (self.mapping_df['è²¨å“åç¨±'].astype(str).str.strip() == product_name_clean)
        )
        
        matched = self.mapping_df[mask]
        if not matched.empty:
            product_code = matched.iloc[0]['è²¨å“ç·¨è™Ÿ']
            logger.debug(f"   ğŸ” æ‰¾åˆ° mapping: {barcode_clean} -> {product_code}")
            return str(product_code).strip()
        
        return None
    
    def add_mapping(self, barcode: str, product_name: str, product_code: str, supplier_name: str = ""):
        """
        æ–°å¢ mapping è¨˜éŒ„
        
        Args:
            barcode: è²¨å“æ¢ç¢¼
            product_name: è²¨å“åç¨±
            product_code: è²¨å“ç·¨è™Ÿ
            supplier_name: ä¾›æ‡‰å•†åç¨±
        """
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        barcode_clean = pd.Series([str(barcode)]).str.strip().str.replace(r'\.0+$', '', regex=True).iloc[0]
        product_name_clean = str(product_name).strip()
        
        mask = (
            (self.mapping_df['è²¨å“æ¢ç¢¼'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True) == barcode_clean) &
            (self.mapping_df['è²¨å“åç¨±'].astype(str).str.strip() == product_name_clean)
        )
        
        if mask.any():
            # æ›´æ–°ç¾æœ‰è¨˜éŒ„
            self.mapping_df.loc[mask, 'è²¨å“ç·¨è™Ÿ'] = str(product_code).strip()
            self.mapping_df.loc[mask, 'ä¾›æ‡‰å•†åç¨±'] = str(supplier_name).strip()
            self.mapping_df.loc[mask, 'å»ºç«‹æ—¥æœŸ'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            logger.info(f"   ğŸ“ æ›´æ–° mapping: {barcode_clean} -> {product_code}")
        else:
            # æ–°å¢è¨˜éŒ„
            new_row = {
                'è²¨å“æ¢ç¢¼': barcode_clean,
                'è²¨å“åç¨±': product_name_clean,
                'è²¨å“ç·¨è™Ÿ': str(product_code).strip(),
                'ä¾›æ‡‰å•†åç¨±': str(supplier_name).strip(),
                'å»ºç«‹æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            self.mapping_df = pd.concat([self.mapping_df, pd.DataFrame([new_row])], ignore_index=True)
            logger.info(f"   â• æ–°å¢ mapping: {barcode_clean} -> {product_code}")
        
        # å„²å­˜åˆ°æª”æ¡ˆ
        self.save_mapping()
    
    def save_mapping(self):
        """å„²å­˜ mapping åˆ°æª”æ¡ˆ"""
        try:
            self.mapping_df.to_excel(self.mapping_file, index=False)
            logger.debug(f"ğŸ’¾ Mapping å·²å„²å­˜: {len(self.mapping_df)} ç­†")
        except Exception as e:
            logger.error(f"âŒ å„²å­˜ mapping å¤±æ•—: {e}")


# --- è®€å–å™¨ ---
class BatchReceiptLoader:
    def __init__(self, base_dir: str = "workspace"):
        self.base_path = Path(base_dir)
        self.pending_path = self.base_path / "pending"
        self.processed_path = self.base_path / "processed"

        # æª¢æŸ¥ä¸¦å»ºç«‹è³‡æ–™å¤¾
        if not self.pending_path.exists() or not self.processed_path.exists():
            self.pending_path.mkdir(parents=True, exist_ok=True)
            self.processed_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“‚ å·¥ä½œç›®éŒ„è¨­å®šå®Œæˆï¼š")
            logger.info(f"   - å¾…è™•ç†: {self.pending_path}")
            logger.info(f"   - å·²æ­¸æª”: {self.processed_path}")


    def get_pending_files(self) -> Generator[Path, None, None]:
        """
        æƒæ 'pending' è³‡æ–™å¤¾ï¼Œæ‰¾å‡ºæ‰€æœ‰ Excel (.xls, .xlsx) å’Œ CSV æª”æ¡ˆ
        """
        # æ”¯æ´çš„å‰¯æª”å
        extensions = ['*.xlsx', '*.xls', '*.csv']
        files_found = False
        
        for ext in extensions:
            # glob æœƒæœå°‹æ‰€æœ‰ç¬¦åˆå‰¯æª”åçš„æª”æ¡ˆ
            for file_path in self.pending_path.glob(ext):
                # å¿½ç•¥ Excel æ‰“é–‹æ™‚ç”¢ç”Ÿçš„æš«å­˜æª” (æª”åä»¥ ~$ é–‹é ­)
                if not file_path.name.startswith('~$'):
                    files_found = True
                    yield file_path
        
        if not files_found:
            logger.warning("âš ï¸ 'pending' è³‡æ–™å¤¾å…§æ²’æœ‰ Excel æˆ– CSV æª”æ¡ˆ")

    def archive_file(self, file_path: Path):
        """æ­¸æª”åŸå§‹æ–‡ä»¶ (è™•ç†æª”æ¡ˆè¢«ä½”ç”¨å•é¡Œ)"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        new_name = f"{file_path.stem}_{timestamp}{file_path.suffix}"
        destination = self.processed_path / new_name

        while True:
            try:
                shutil.move(str(file_path), str(destination))
                logger.info(f"ğŸ“¦ å·²æ­¸æª”: {new_name}")
                break # æˆåŠŸç§»å‹•å¾Œè·³å‡ºè¿´åœˆ
            except PermissionError:
                logger.warning(f"âš ï¸ ç„¡æ³•ç§»å‹•æª”æ¡ˆ (è¢«ä½”ç”¨): {file_path.name}")
                logger.error(f"ğŸ›‘ éŒ¯èª¤ï¼šæª”æ¡ˆ '{file_path.name}' æ­£è¢« Excel é–‹å•Ÿä¸­ï¼")
                logger.info("ğŸ‘‰ è«‹é—œé–‰è©²æª”æ¡ˆï¼Œç„¶å¾ŒæŒ‰ [Enter] éµé‡è©¦...")
                input()  # ç­‰å¾…ç”¨æˆ¶è¼¸å…¥ï¼Œä½†ä¸è¼¸å‡ºåˆ°çµ‚ç«¯ï¼ˆé€šé logger å·²è¨˜éŒ„ï¼‰
                logger.info("ğŸ”„ ä½¿ç”¨è€…å˜—è©¦é‡è©¦æ­¸æª”...")
            except Exception as e:
                logger.error(f"âŒ æ­¸æª”å¤±æ•— (æœªçŸ¥éŒ¯èª¤): {e}")
                break # å…¶ä»–éŒ¯èª¤ç›´æ¥æ”¾æ£„ï¼Œé¿å…ç„¡çª®è¿´åœˆ

    def smart_load(self, file_path: Path, expected_keywords: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:
        try:
            logger.info(f"ğŸ“– è®€å–: {file_path.name}")
            # 1. å…ˆè®€å‰ 20 è¡Œç”¨ä¾†æ‰¾ Header
            if file_path.suffix == '.csv':
                 df_raw = pd.read_csv(file_path, header=None, nrows=20)
            else:
                df_raw = pd.read_excel(file_path, header=None, nrows=20)
            
            header_idx = self._find_header_row(df_raw, expected_keywords)
            
            if header_idx == -1:
                logger.warning(f"âš ï¸ {file_path.name}: æ‰¾ä¸åˆ°æ¨™é¡Œåˆ—ï¼Œè·³é")
                return pd.DataFrame(), pd.DataFrame()

            # 2. æ­£å¼è®€å–æ•¸æ“š
            # ä¿®æ”¹ï¼šåŠ å…¥ dtype=str å¼·åˆ¶æ‰€æœ‰æ¬„ä½ä»¥ã€Œç´”æ–‡å­—ã€è®€å–ï¼Œä¿ç•™é–‹é ­çš„ 0ï¼Œé¿å…è®Šæˆæ•¸å­—
            if file_path.suffix == '.csv':
                df_data = pd.read_csv(file_path, header=header_idx, dtype=str)
            else:
                df_data = pd.read_excel(file_path, header=header_idx, dtype=str)
                
            df_data = df_data.dropna(how='all', axis=0).dropna(how='all', axis=1)
            return df_raw, df_data

        except Exception as e:
            logger.error(f"âŒ è®€å–éŒ¯: {e}")
            return pd.DataFrame(), pd.DataFrame()

    def _find_header_row(self, df: pd.DataFrame, keywords: List[str]) -> int:
        max_score = 0
        best_idx = -1
        for idx, row in df.iterrows():
            row_str = " ".join(row.astype(str)).lower()
            score = 0
            for key in keywords:
                # åªè¦å‘½ä¸­å…¶ä¸­ä¸€å€‹é—œéµå­—
                if key and key.lower() in row_str:
                    score += 1
            
            # ä¿®æ­£ï¼šå¿…é ˆå‘½ä¸­è‡³å°‘ 2 å€‹é—œéµå­—æ‰ç®—æ‰¾åˆ°
            if score > max_score and score >= 2:
                max_score = score
                best_idx = idx
        return best_idx




# --- æ¸…æ´—å™¨ ---
class ReceiptCleaner:
    def __init__(self, config_df: pd.DataFrame):
        self.config_df = config_df
        
    def identify_supplier_by_columns(self, file_columns: List[str]) -> Tuple[Optional[pd.Series], str]:
        """
        æ ¸å¿ƒé‚è¼¯ï¼šæª¢æŸ¥æ”¶æ“šçš„æ¨™é¡Œåˆ—ï¼Œæ˜¯å¦åŒ…å«è¨­å®šæª”ä¸­æŸå®¶å» å•†çš„æ‰€æœ‰ 4 å€‹é—œéµæ¬„ä½
        """
        if self.config_df.empty:
            return None, "Unknown"

        file_cols_lower = {str(c).strip().lower() for c in file_columns}

        for _, row in self.config_df.iterrows():
            required_cols = [
                str(row['è²¨å“æ¢ç¢¼']).strip().lower(),
                str(row['å…¥è²¨åƒ¹']).strip().lower(),
                str(row['å…¥è²¨é‡']).strip().lower(),
                str(row['è²¨å“åç¨±']).strip().lower()
            ]
            supplier_name = row['ä¾›æ‡‰å•†åç¨±']

            # æª¢æŸ¥æ˜¯å¦ 4 å€‹æ¬„ä½éƒ½å­˜åœ¨
            if set(required_cols).issubset(file_cols_lower):
                logger.info(f"   ğŸ¯ æ¬„ä½å®Œå…¨åŒ¹é… -> è­˜åˆ¥ç‚º: {supplier_name}")
                return row, supplier_name
        
        return None, "New Supplier"

    def process(self, df: pd.DataFrame) -> Tuple[Optional[pd.DataFrame], str]:
        """
        è™•ç†æ”¶æ“šæ•¸æ“š
        
        Returns:
            Tuple[Optional[pd.DataFrame], str]: 
            - DataFrame: æ¸…æ´—å¾Œçš„æ•¸æ“šï¼Œå¦‚æœè™•ç†å¤±æ•—å‰‡è¿”å› None
            - str: è­˜åˆ¥åˆ°çš„ä¾›æ‡‰å•†åç¨±ï¼Œå¦‚æœç„¡æ³•è­˜åˆ¥å‰‡è¿”å›ç©ºå­—ä¸²
        """
        # 1. è­˜åˆ¥
        supplier_config, supplier_name = self.identify_supplier_by_columns(df.columns)
        
        # 2. æ”¹å (åªæœ‰è­˜åˆ¥æˆåŠŸæ‰æ”¹åï¼Œä¸äº‚çŒœ)
        if supplier_config is not None:
            df = self._rename_columns_strict(df, supplier_config)
        else:
            logger.warning("   âš ï¸ ç„¡æ³•è­˜åˆ¥ä¾›æ‡‰å•† (æ¬„ä½ç‰¹å¾µä¸ç¬¦)")
            logger.info(f"      æ”¶æ“šæ¬„ä½: {list(df.columns)}")
            return None, "" # ç›´æ¥è¿”å› Noneï¼Œä¸ç¹¼çºŒè™•ç†

        # 3. æª¢æŸ¥å¿…è¦æ¬„ä½
        required_cols = ["è²¨å“æ¢ç¢¼", "å…¥è²¨åƒ¹", "å…¥è²¨é‡", "è²¨å“åç¨±"]
        missing = [c for c in required_cols if c not in df.columns]
        if missing:
            logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
            return None, supplier_name

        df = df[required_cols].copy()
        
        # 4. åŸºç¤æ¸…æ´—
        # ä¿®æ”¹ï¼šæ­£å‰‡è¡¨é”å¼æ”¹ç‚º r'\.0+$'ï¼Œæ„æ€æ˜¯ã€Œå°æ•¸é»å¾Œè·Ÿè‘—ä¸€å€‹æˆ–å¤šå€‹ 0ã€éƒ½è¦åˆªæ‰
        # é€™æ¨£å¯ä»¥åŒæ™‚è™•ç† .0, .00, .000 ç­‰æƒ…æ³
        df['è²¨å“æ¢ç¢¼'] = df['è²¨å“æ¢ç¢¼'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True)
        for col in ['å…¥è²¨åƒ¹', 'å…¥è²¨é‡']:
            df[col] = df[col].astype(str).str.replace(r'[^\d.]', '', regex=True)
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        df['å…¥è²¨é‡'] = df['å…¥è²¨é‡'].astype(int)
        
        # 5. æˆæœ¬åŠ æˆé‚è¼¯
        multiplier_flag = str(supplier_config.get('æˆæœ¬ä¹˜ä»¥1.2', '')).strip()
        if multiplier_flag in ['æ˜¯', 'Yes', 'TRUE', 'True', '1']:
            logger.info("   ğŸ’° åŸ·è¡Œæˆæœ¬åŠ æˆ (x1.2)")
            df['å…¥è²¨åƒ¹'] = df['å…¥è²¨åƒ¹'] * 1.2
            df['å…¥è²¨åƒ¹'] = df['å…¥è²¨åƒ¹'].round(2)

        # 6. è£œé½Šæ¬„ä½
        df['ä¾›æ‡‰å•†åç¨±'] = supplier_name  # å¡«å…¥è­˜åˆ¥åˆ°çš„ä¾›æ‡‰å•†åç¨±
        df['åº—è™Ÿ'] = 'S1'
        df['å…¥è²¨æ—¥æœŸ'] = datetime.now().strftime('%Y%m%d')
        df['æ”¶æ“šå–®è™Ÿ'] = ''
        df['ä¾›æ‡‰å•†ç·¨è™Ÿ'] = '001'
        df['å‚™è¨»'] = ''
        df['ç‹€æ…‹'] = ''
        df['è²¨å“ç·¨è™Ÿ'] = df['è²¨å“æ¢ç¢¼']

        df = df[ (df['è²¨å“æ¢ç¢¼'] != '') & (df['è²¨å“æ¢ç¢¼'] != 'nan') & (df['å…¥è²¨é‡'] > 0) ]
        return df, supplier_name

    def _rename_columns_strict(self, df: pd.DataFrame, config: pd.Series) -> pd.DataFrame:
        """æ ¹æ“šè¨­å®šæª”ç²¾ç¢ºæ”¹å"""
        target_map = {
            "è²¨å“æ¢ç¢¼": config.get("è²¨å“æ¢ç¢¼"),
            "å…¥è²¨åƒ¹": config.get("å…¥è²¨åƒ¹"),
            "å…¥è²¨é‡": config.get("å…¥è²¨é‡"),
            "è²¨å“åç¨±": config.get("è²¨å“åç¨±")
        }

        reverse_map = {}
        for target, source in target_map.items():
            if pd.notna(source):
                reverse_map[str(source).strip().lower()] = target
        
        new_columns = {}
        for col in df.columns:
            if str(col).strip().lower() in reverse_map:
                new_columns[col] = reverse_map[str(col).strip().lower()]
        
        return df.rename(columns=new_columns)


# --- ç”¢å“é©—è­‰å™¨ ---
class ProductValidator:
    def __init__(self, stock_csv_path: str, mapping_manager: Optional['MappingManager'] = None):
        """
        åˆå§‹åŒ–ç”¢å“é©—è­‰å™¨ï¼Œè®€å– POS åº«å­˜è¨˜éŒ„
        
        Args:
            stock_csv_path: DetailGoodsStockToday.csv çš„è·¯å¾‘
            mapping_manager: MappingManager å¯¦ä¾‹ï¼Œç”¨æ–¼æª¢æŸ¥å·²è¨˜éŒ„çš„ mapping
        """
        self.stock_csv_path = Path(stock_csv_path)
        self.productcode_set = set()
        self.barcode_set = set()
        self.mapping_manager = mapping_manager
        self.stock_df = None  # å„²å­˜å®Œæ•´çš„åº«å­˜æ•¸æ“šï¼Œç”¨æ–¼æŸ¥æ‰¾å…±ç”¨æ¢ç¢¼çš„é¸é …
        self._load_stock_data()
    
    def _load_stock_data(self):
        """è®€å–åº«å­˜ CSV ä¸¦å»ºç«‹æŸ¥æ‰¾é›†åˆ"""
        if not self.stock_csv_path.exists():
            logger.warning(f"âš ï¸ åº«å­˜æª”æ¡ˆä¸å­˜åœ¨: {self.stock_csv_path}")
            return
        
        try:
            # è®€å– CSVï¼Œä½¿ç”¨å­—ä¸²é¡å‹é¿å…æ ¼å¼å•é¡Œ
            df = pd.read_csv(self.stock_csv_path, dtype=str, encoding='utf-8-sig')
            self.stock_df = df  # å„²å­˜å®Œæ•´æ•¸æ“š
            
            # æå– ProductCode å’Œ Barcode æ¬„ä½
            if 'ProductCode' in df.columns:
                # æ¸…æ´—ä¸¦è½‰æ›ç‚ºé›†åˆï¼šå»é™¤ç©ºç™½ã€è™•ç† .0 çµå°¾ã€éæ¿¾ç©ºå€¼
                productcodes = df['ProductCode'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True)
                self.productcode_set = {code for code in productcodes if code and code.lower() != 'nan'}
            
            if 'Barcode' in df.columns:
                # æ¸…æ´—ä¸¦è½‰æ›ç‚ºé›†åˆï¼šå»é™¤ç©ºç™½ã€è™•ç† .0 çµå°¾ã€éæ¿¾ç©ºå€¼
                barcodes = df['Barcode'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True)
                self.barcode_set = {code for code in barcodes if code and code.lower() != 'nan'}
            
            logger.info(f"âœ… å·²è¼‰å…¥åº«å­˜è¨˜éŒ„: ProductCode {len(self.productcode_set)} ç­†, Barcode {len(self.barcode_set)} ç­†")
            
        except Exception as e:
            logger.error(f"âŒ è®€å–åº«å­˜æª”æ¡ˆå¤±æ•—: {e}")
    
    def get_barcode_options(self, barcode: str) -> List[Dict[str, str]]:
        """
        ç²å–å…±ç”¨æ¢ç¢¼å°æ‡‰çš„æ‰€æœ‰ ProductCode é¸é …
        
        Args:
            barcode: æ¢ç¢¼
        
        Returns:
            åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å« ProductCode å’Œ Name
        """
        if self.stock_df is None or 'Barcode' not in self.stock_df.columns:
            return []
        
        barcode_clean = pd.Series([str(barcode)]).str.strip().str.replace(r'\.0+$', '', regex=True).iloc[0]
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„è¨˜éŒ„
        mask = self.stock_df['Barcode'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True) == barcode_clean
        matched = self.stock_df[mask]
        
        options = []
        for _, row in matched.iterrows():
            product_code = str(row.get('ProductCode', '')).strip()
            name = str(row.get('Name', '')).strip()
            if product_code and product_code.lower() != 'nan':
                options.append({'ProductCode': product_code, 'Name': name})
        
        return options
    
    def validate_products(self, df: pd.DataFrame, supplier_name: str = "") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        é©—è­‰ç”¢å“æ˜¯å¦å­˜åœ¨æ–¼ POS ç³»çµ±ä¸­
        
        Args:
            df: æ¸…æ´—å¾Œçš„æ”¶æ“š DataFrameï¼Œå¿…é ˆåŒ…å«ã€Œè²¨å“æ¢ç¢¼ã€æ¬„ä½
            supplier_name: ä¾›æ‡‰å•†åç¨±ï¼Œç”¨æ–¼ mapping æŸ¥æ‰¾
        
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: 
            - matched_df: æ‰¾åˆ° ProductCode å°æ‡‰çš„è¨˜éŒ„ï¼ˆæ­£å¸¸è™•ç†ï¼‰
            - unmatched_df: æ‰¾ä¸åˆ°å°æ‡‰çš„è¨˜éŒ„ï¼ŒåŒ…å«ã€Œè™•ç†åŸå› ã€æ¬„ä½
        """
        if df.empty or 'è²¨å“æ¢ç¢¼' not in df.columns:
            return pd.DataFrame(), pd.DataFrame()
        
        # è¤‡è£½ DataFrame ä»¥é¿å…ä¿®æ”¹åŸå§‹è³‡æ–™
        df = df.copy()
        
        # æº–å‚™æ¯”å°ç”¨çš„æ¢ç¢¼ï¼ˆæ¸…æ´—æ ¼å¼ï¼‰
        df['_barcode_clean'] = df['è²¨å“æ¢ç¢¼'].astype(str).str.strip().str.replace(r'\.0+$', '', regex=True)
        
        # åˆå§‹åŒ–åŒ¹é…ç‹€æ…‹å’ŒåŸå› å­—å…¸
        matched_mask = pd.Series([False] * len(df), index=df.index)
        reason_dict = {}  # ä½¿ç”¨å­—å…¸å„²å­˜æ¯å€‹ç´¢å¼•å°æ‡‰çš„åŸå› 
        
        # çµ±è¨ˆç”¨
        mapping_count = 0
        matched_count = 0
        barcode_only_count = 0
        unmatched_count = 0
        
        for idx, row in df.iterrows():
            barcode = row['_barcode_clean']
            product_name = str(row.get('è²¨å“åç¨±', '')).strip()
            
            # è·³éç©ºå€¼
            if not barcode or barcode.lower() == 'nan':
                continue
            
            # å„ªå…ˆæª¢æŸ¥ mappingï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if self.mapping_manager:
                mapped_product_code = self.mapping_manager.find_mapping(barcode, product_name)
                if mapped_product_code:
                    # æ‰¾åˆ° mappingï¼Œç›´æ¥ä½¿ç”¨
                    matched_mask[idx] = True
                    # æ›´æ–°è²¨å“ç·¨è™Ÿ
                    df.loc[idx, 'è²¨å“ç·¨è™Ÿ'] = mapped_product_code
                    mapping_count += 1
                    continue
            
            # æƒ…æ³1: æ‰¾åˆ° ProductCodeï¼ˆå®Œå…¨åŒ¹é…ï¼‰
            if barcode in self.productcode_set:
                matched_mask[idx] = True
                matched_count += 1
            # æƒ…æ³2: åªæ‰¾åˆ° Barcodeï¼ˆéƒ¨åˆ†åŒ¹é…ï¼‰
            elif barcode in self.barcode_set:
                reason_dict[idx] = 'å…±ç”¨æ¢ç¢¼ï¼Œéœ€äººæ‰‹é¸æ“‡é¡è‰²æˆ–å¤§å°'
                barcode_only_count += 1
            # æƒ…æ³3: å®Œå…¨æ‰¾ä¸åˆ°
            else:
                reason_dict[idx] = 'å¯èƒ½æ˜¯æ¢ç¢¼éŒ¯èª¤æˆ–æ–°è²¨å“'
                unmatched_count += 1
        
        # åˆ†é›¢æ•¸æ“š
        matched_df = df[matched_mask].copy()
        unmatched_df = df[~matched_mask].copy()
        
        # ç§»é™¤è‡¨æ™‚æ¬„ä½
        if '_barcode_clean' in matched_df.columns:
            matched_df = matched_df.drop(columns=['_barcode_clean'])
        if '_barcode_clean' in unmatched_df.columns:
            unmatched_df = unmatched_df.drop(columns=['_barcode_clean'])
        
        # ç‚º unmatched_df åŠ å…¥è™•ç†åŸå› æ¬„ä½
        if not unmatched_df.empty:
            # ä½¿ç”¨ç´¢å¼•å°æ‡‰çš„åŸå› 
            unmatched_df['è™•ç†åŸå› '] = unmatched_df.index.map(reason_dict).fillna('')
        
        # è¨˜éŒ„çµ±è¨ˆ
        logger.info(f"   ğŸ“Š ç”¢å“é©—è­‰çµæœ:")
        if mapping_count > 0:
            logger.info(f"      ğŸ”„ ä½¿ç”¨ Mapping: {mapping_count} ç­†")
        logger.info(f"      âœ… æ‰¾åˆ° ProductCode: {matched_count} ç­†")
        logger.info(f"      âš ï¸ åªæ‰¾åˆ° Barcode: {barcode_only_count} ç­†")
        logger.info(f"      âŒ å®Œå…¨æ‰¾ä¸åˆ°: {unmatched_count} ç­†")
        
        return matched_df, unmatched_df


# --- æª”æ¡ˆè¼¸å‡ºå™¨ ---
class ReceiptExporter:
    def __init__(self, base_dir: str = "workspace"):
        self.output_root = Path(base_dir) / "output"

        self.pos_dir = self.output_root / "pos_import" # å­˜æ”¾ POS æ ¼å¼çš„ XLS
        
        self.pos_dir.mkdir(parents=True, exist_ok=True)

    def save_pos_excel(self, df: pd.DataFrame, original_filename: str):
        """ç”¢ç”Ÿ POS ç³»çµ±å°ˆç”¨çš„èˆŠç‰ˆ .xls æ ¼å¼ (å­—ä¸²æ¨¡å¼)"""
        # å®šç¾©æ¨¡æ¿ (ä¾ç…§ä½ çš„éœ€æ±‚)
        # éµå€¼ (0-12) å°æ‡‰ Excel çš„ç¬¬ A-M æ¬„
        Instock_template = {
            0: [
                'MBA POS å…¥è²¨è¡¨', '', 
                'è«‹ä¸è¦ä¿®æ”¹æ­¤å…¥è²¨è¡¨ä¹‹æ ¼å¼!! å¦‚æ­¤å…¥è²¨è¡¨ä¹‹æ ¼å¼è¢«ä¿®æ”¹, å¯èƒ½æœƒå°è‡´ç³»çµ±ä¸èƒ½åŒ¯å…¥æ­¤è¡¨ä¸­çš„è³‡æ–™!!', 
                'ç³»çµ±åªæœƒæª¢æŸ¥ä¸¦æŠŠ è²¨è™Ÿ/ æ¢ç¢¼,  å…¥è²¨åƒ¹,  å…¥è²¨é‡, åº—è™Ÿ, å…¥è²¨æ—¥æœŸ, æ”¶æ“šå–®è™Ÿ åŠ ä¾›æ‡‰å•†ç·¨è™Ÿ  è³‡æ–™åŒ¯å…¥ç³»çµ±.  è²¨å“åŠä¾›æ‡‰å•†åç¨±åªä¾›å®¢äººä½œåƒè€ƒ.', 
                'å¯åœ¨ä¸‹è¡¨ åªè¼¸å…¥è²¨å“ç·¨è™Ÿæˆ–è²¨ å“æ¢ç¢¼.   å¦‚åœ¨ä¸‹è¡¨åŒæ™‚è¼¸å…¥è²¨å“ç·¨è™ŸåŠè²¨å“æ¢ç¢¼, ç³»çµ±æœƒä»¥è²¨å“ç·¨è™Ÿç‚ºæº–.', 
                'è«‹ç•™æ„, å…¥è²¨æ—¥æœŸæ ¼å¼ç‚º (å¹´å¹´å¹´å¹´æœˆæœˆæ—¥æ—¥), å³ä»Šå¤©çš„æ—¥æœŸç‚º 20250315', 
                '', '', '', '', '', '', '', '', 'è²¨å“ç·¨è™Ÿ'
            ],
            1: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'è²¨å“æ¢ç¢¼'],
            2: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'è²¨å“åç¨±'],
            3: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'å…¥è²¨åƒ¹'],
            4: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'å…¥è²¨é‡'],
            5: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'åº—è™Ÿ'],
            6: ['Ver:3.2', '', '', '', '', '', '', '', '', '', '', '', '', '', 'å…¥è²¨æ—¥æœŸ'],
            7: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'æ”¶æ“šå–®è™Ÿ'],
            8: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ä¾›æ‡‰å•†ç·¨è™Ÿ'],
            9: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ä¾›æ‡‰å•†åç¨±'],
            10: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'å‚™è¨»'],
            11: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ç‹€æ…‹'],
            12: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ç³»çµ±å‚™è¨»']
        }

        # 1. æº–å‚™ POS ç³»çµ±è¦æ±‚çš„æ¬„ä½é †åº (å‹™å¿…èˆ‡ Template Key 0~12 å°æ‡‰)
        target_columns = [
            'è²¨å“ç·¨è™Ÿ', 'è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'å…¥è²¨åƒ¹', 'å…¥è²¨é‡',
            'åº—è™Ÿ', 'å…¥è²¨æ—¥æœŸ', 'æ”¶æ“šå–®è™Ÿ', 'ä¾›æ‡‰å•†ç·¨è™Ÿ', 'ä¾›æ‡‰å•†åç¨±',
            'å‚™è¨»', 'ç‹€æ…‹', 'ç³»çµ±å‚™è¨»'
        ]

        # ç¢ºä¿ DataFrame æŒ‰ç…§é€™å€‹é †åºæ’åˆ—ï¼Œç¼ºå°‘çš„æ¬„ä½æœƒåœ¨ Cleaner éšæ®µè£œé½Š
        # è‹¥æœ‰è¬ä¸€ç¼ºå°‘çš„ï¼Œé€™è£¡è£œä¸Šç©ºå­—ä¸²ä»¥å…å ±éŒ¯
        for col in target_columns:
            if col not in df.columns:
                df[col] = ''
        
        df_export = df[target_columns].copy()

        # 2. å»ºç«‹ Excel
        workbook = xlwt.Workbook(encoding='utf-8')
        sheet = workbook.add_sheet('Sheet1')

        # 3. å¯«å…¥æ¨¡æ¿ (Template)
        # Template çµæ§‹ï¼šKey æ˜¯ Column Indexï¼ŒValue æ˜¯è©² Column çš„ Rows List
        for col_idx, row_data_list in Instock_template.items():
            for row_idx, cell_value in enumerate(row_data_list):
                # å¼·åˆ¶è½‰å­—ä¸²
                sheet.write(row_idx, col_idx, str(cell_value))

        # 4. å¯«å…¥æ•¸æ“š (Data)
        # è³‡æ–™å¾æ¨¡æ¿æœ€é•·çš„é‚£ä¸€è¡Œä¹‹å¾Œé–‹å§‹å¯«å…¥
        start_row = max(len(row) for row in Instock_template.values())
        
        for i, row_data in enumerate(df_export.values):
            for j, cell_value in enumerate(row_data):
                # å¼·åˆ¶è½‰å­—ä¸² (str)ï¼Œç¢ºä¿ POS ç³»çµ±ç›¸å®¹æ€§
                sheet.write(start_row + i, j, str(cell_value))

        # 5. å­˜æª”
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"POS_{Path(original_filename).stem}_{timestamp}.xls"
        save_path = self.pos_dir / filename
        
        workbook.save(str(save_path))
        logger.info(f"   ğŸ’¾ POS åŒ¯å…¥æª”: {filename}")
    
    def save_unmatched_excel(self, df: pd.DataFrame, supplier_name: str, validator: Optional['ProductValidator'] = None, base_dir: str = "workspace"):
        """
        å°‡æ‰¾ä¸åˆ°å°æ‡‰çš„ç”¢å“å­˜æˆå¾…è™•ç† Excel æª” (.xlsx)
        
        Args:
            df: åŒ…å«ã€Œè™•ç†åŸå› ã€æ¬„ä½çš„å¾…è™•ç† DataFrame
            supplier_name: è­˜åˆ¥åˆ°çš„ä¾›æ‡‰å•†åç¨±
            validator: ProductValidator å¯¦ä¾‹ï¼Œç”¨æ–¼ç²å–å…±ç”¨æ¢ç¢¼çš„é¸é …
            base_dir: å·¥ä½œç›®éŒ„
        """
        if df.empty:
            return
        
        # ç¢ºä¿ pending è³‡æ–™å¤¾å­˜åœ¨
        pending_dir = Path(base_dir) / "pending"
        pending_dir.mkdir(parents=True, exist_ok=True)
        
        # ç°¡åŒ–è¼¸å‡ºæ¬„ä½
        output_columns = [
            'è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'å…¥è²¨åƒ¹', 'å…¥è²¨é‡', 'è™•ç†åŸå› ', 'äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ'
        ]
        
        # æº–å‚™è¼¸å‡º DataFrame
        # ç¢ºä¿åŸºæœ¬æ¬„ä½å­˜åœ¨
        base_cols = ['è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'å…¥è²¨åƒ¹', 'å…¥è²¨é‡']
        missing_base_cols = [col for col in base_cols if col not in df.columns]
        if missing_base_cols:
            logger.error(f"âŒ å¾…è™•ç†æª”ç¼ºå°‘åŸºæœ¬æ¬„ä½: {missing_base_cols}")
            logger.error(f"   ç¾æœ‰æ¬„ä½: {list(df.columns)}")
            logger.error(f"   DataFrame è¡Œæ•¸: {len(df)}")
            raise ValueError(f"ç¼ºå°‘åŸºæœ¬æ¬„ä½: {missing_base_cols}")
        
        # æª¢æŸ¥ DataFrame æ˜¯å¦ç‚ºç©ºï¼ˆåœ¨æª¢æŸ¥æ¬„ä½å¾Œï¼‰
        if len(df) == 0:
            logger.warning("âš ï¸ DataFrame ç‚ºç©ºï¼Œç„¡æ³•ä¿å­˜")
            return
        
        df_export = df[base_cols].copy()
        logger.debug(f"   æº–å‚™ä¿å­˜ {len(df_export)} ç­†è¨˜éŒ„")
        
        # ä¿ç•™æˆ–æ–°å¢ã€Œè™•ç†åŸå› ã€æ¬„ä½
        if 'è™•ç†åŸå› ' in df.columns:
            df_export['è™•ç†åŸå› '] = df['è™•ç†åŸå› ']
            logger.debug(f"   å·²ä¿ç•™ã€Œè™•ç†åŸå› ã€æ¬„ä½")
        else:
            df_export['è™•ç†åŸå› '] = ''
            logger.debug(f"   æ–°å¢ã€Œè™•ç†åŸå› ã€æ¬„ä½ï¼ˆç©ºå€¼ï¼‰")
        
        # ã€Œäººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿã€æ¬„ä½ï¼šå¦‚æœæ˜¯é‡æ–°ä¿å­˜æœªå¡«å¯«çš„è¨˜éŒ„ï¼Œç¢ºä¿æ˜¯ç©ºç™½
        # æª¢æŸ¥æ˜¯å¦æœ‰å·²å¡«å¯«çš„å€¼ï¼Œå¦‚æœæœ‰å‰‡æ¸…ç©ºï¼ˆå› ç‚ºé€™äº›æ˜¯æœªå¡«å¯«çš„è¨˜éŒ„ï¼‰
        if 'äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ' in df.columns:
            # åªä¿ç•™çœŸæ­£ç‚ºç©ºçš„è¨˜éŒ„ï¼ˆæœªå¡«å¯«çš„ï¼‰
            df_export['äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ'] = ''
            logger.debug(f"   å·²ä¿ç•™ã€Œäººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿã€æ¬„ä½ï¼ˆè¨­ç‚ºç©ºï¼‰")
        else:
            df_export['äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ'] = ''  # ç©ºç™½æ¬„ä½ï¼Œä¾›äººå·¥å¡«å¯«
            logger.debug(f"   æ–°å¢ã€Œäººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿã€æ¬„ä½ï¼ˆç©ºå€¼ï¼‰")
        
        # å­˜æª” - ä½¿ç”¨æ–°æ ¼å¼ï¼š{ä¾›æ‡‰å•†åç¨±}éœ€è¦äººæ‰‹è™•ç†{æ—¥æœŸ}.xlsx
        date_str = datetime.now().strftime("%Y%m%d")
        # æ¸…ç†ä¾›æ‡‰å•†åç¨±ï¼Œç§»é™¤å¯èƒ½å°è‡´æª”æ¡ˆåå•é¡Œçš„å­—å…ƒ
        safe_supplier_name = supplier_name.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
        filename = f"{safe_supplier_name}éœ€è¦äººæ‰‹è™•ç†{date_str}.xlsx"
        save_path = pending_dir / filename
        
        try:
            # å…ˆä½¿ç”¨ pandas è¼¸å‡ºåŸºæœ¬æ•¸æ“š
            logger.debug(f"   æ­£åœ¨ä¿å­˜åˆ°: {save_path}")
            df_export.to_excel(save_path, index=False, engine='openpyxl')
            logger.debug(f"   åŸºæœ¬æ•¸æ“šå·²ä¿å­˜ï¼Œå…± {len(df_export)} ç­†è¨˜éŒ„")
            
            # ä½¿ç”¨ openpyxl æ·»åŠ è¨»è§£
            try:
                from openpyxl import load_workbook
                from openpyxl.comments import Comment
                
                wb = load_workbook(save_path)
                ws = wb.active
                
                # æ‰¾åˆ°ã€Œäººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿã€æ¬„ä½çš„ç´¢å¼•
                product_code_col_idx = output_columns.index('äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ') + 1  # Excel å¾ 1 é–‹å§‹
                
                # ç‚ºæ¯ä¸€è¡Œæ·»åŠ è¨»è§£ï¼ˆå¦‚æœæ˜¯å…±ç”¨æ¢ç¢¼ï¼‰
                # ä½¿ç”¨ enumerate ä¾†ç²å–å¯¦éš›çš„è¡Œè™Ÿï¼ˆå¾ 0 é–‹å§‹ï¼Œå°æ‡‰ Excel çš„ç¬¬ 2 è¡Œé–‹å§‹ï¼Œå› ç‚ºç¬¬ 1 è¡Œæ˜¯æ¨™é¡Œï¼‰
                comment_count = 0
                for excel_row_idx, (df_idx, row) in enumerate(df_export.iterrows(), start=2):
                    barcode = str(row.get('è²¨å“æ¢ç¢¼', '')).strip()
                    reason = str(row.get('è™•ç†åŸå› ', '')).strip()
                    
                    # å¦‚æœæ˜¯å…±ç”¨æ¢ç¢¼ï¼Œæ·»åŠ é¸é …è¨»è§£
                    if reason == 'å…±ç”¨æ¢ç¢¼ï¼Œéœ€äººæ‰‹é¸æ“‡é¡è‰²æˆ–å¤§å°' and validator:
                        options = validator.get_barcode_options(barcode)
                        if options:
                            # å»ºç«‹è¨»è§£å…§å®¹
                            comment_text = "å¯é¸çš„è²¨å“ç·¨è™Ÿï¼š\n"
                            for opt in options:
                                product_code = opt.get('ProductCode', '')
                                name = opt.get('Name', '')
                                comment_text += f"- {product_code}: {name}\n"
                            
                            # æ·»åŠ è¨»è§£åˆ°å°æ‡‰çš„å„²å­˜æ ¼ï¼ˆexcel_row_idx å·²ç¶“æ˜¯æ­£ç¢ºçš„è¡Œè™Ÿï¼Œå¾ 2 é–‹å§‹ï¼‰
                            cell = ws.cell(row=excel_row_idx, column=product_code_col_idx)
                            cell.comment = Comment(comment_text, "ç³»çµ±")
                            cell.comment.width = 300
                            cell.comment.height = 100
                            comment_count += 1
                
                wb.save(save_path)
                if comment_count > 0:
                    logger.debug(f"   å·²æ·»åŠ  {comment_count} å€‹è¨»è§£")
                logger.info(f"   ğŸ“‹ å¾…è™•ç†æª”: {filename} (å·²åŠ å…¥è¨»è§£)")
            except ImportError:
                logger.warning("   âš ï¸ ç„¡æ³•æ·»åŠ è¨»è§£ï¼ˆéœ€è¦ openpyxlï¼‰ï¼Œä½†æª”æ¡ˆå·²å„²å­˜")
            except Exception as e:
                logger.warning(f"   âš ï¸ æ·»åŠ è¨»è§£æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ï¼Œä½†æª”æ¡ˆå·²å„²å­˜")
            
            logger.info(f"   ğŸ“‹ å¾…è™•ç†æª”: {filename}")
            if 'è™•ç†åŸå› ' in df_export.columns:
                reason_counts = df_export['è™•ç†åŸå› '].value_counts().to_dict()
                logger.info(f"      åŸå› çµ±è¨ˆ: {reason_counts}")
            logger.info(f"      æª”æ¡ˆè·¯å¾‘: {save_path}")
        except ImportError:
            # å¦‚æœæ²’æœ‰ openpyxlï¼Œå˜—è©¦ä½¿ç”¨ xlsxwriterï¼ˆä½†ç„¡æ³•æ·»åŠ è¨»è§£ï¼‰
            try:
                df_export.to_excel(save_path, index=False, engine='xlsxwriter')
                logger.warning("   âš ï¸ ä½¿ç”¨ xlsxwriter å„²å­˜ï¼ˆç„¡æ³•æ·»åŠ è¨»è§£ï¼‰ï¼Œå»ºè­°å®‰è£ openpyxl")
                logger.info(f"   ğŸ“‹ å¾…è™•ç†æª”: {filename}")
                logger.info(f"      åŸå› çµ±è¨ˆ: {df_export['è™•ç†åŸå› '].value_counts().to_dict()}")
            except ImportError:
                logger.error("âŒ éœ€è¦å®‰è£ openpyxl æˆ– xlsxwriter æ‰èƒ½è¼¸å‡º .xlsx æ ¼å¼")
                logger.info("   è«‹åŸ·è¡Œ: pip install openpyxl")
        except Exception as e:
            logger.error(f"âŒ å„²å­˜å¾…è™•ç†æª”å¤±æ•—: {e}")
            logger.error(f"   æª”æ¡ˆè·¯å¾‘: {save_path}")
            logger.error(f"   è³‡æ–™ç­†æ•¸: {len(df_export)}")
            raise  # é‡æ–°æ‹‹å‡ºç•°å¸¸ï¼Œè®“ä¸»æµç¨‹çš„ try-except èƒ½æ•ç²
    
    def process_manual_excel(self, file_path: Path, mapping_manager: 'MappingManager', validator: 'ProductValidator', base_dir: str = "workspace") -> Tuple[pd.DataFrame, int, pd.DataFrame]:
        """
        è™•ç†äººå·¥å¡«å¯«çš„å¾…è™•ç† Excel æª”æ¡ˆ
        
        Args:
            file_path: å¾…è™•ç† Excel æª”æ¡ˆè·¯å¾‘
            mapping_manager: MappingManager å¯¦ä¾‹
            validator: ProductValidator å¯¦ä¾‹
            base_dir: å·¥ä½œç›®éŒ„
        
        Returns:
            Tuple[pd.DataFrame, int, pd.DataFrame]: 
            - è™•ç†å¾Œçš„ DataFrameï¼ˆå·²å¡«å¯«çš„è¨˜éŒ„ï¼‰
            - æ–°å¢çš„ mapping æ•¸é‡
            - æœªå¡«å¯«çš„ DataFrameï¼ˆéœ€è¦ä¿ç•™çš„è¨˜éŒ„ï¼‰
        """
        try:
            # è®€å– Excel
            df = pd.read_excel(file_path, dtype=str, engine='openpyxl')
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_cols = ['è²¨å“æ¢ç¢¼', 'è²¨å“åç¨±', 'äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ']
            missing = [c for c in required_cols if c not in df.columns]
            if missing:
                logger.error(f"âŒ å¾…è™•ç†æª”ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
                return pd.DataFrame(), 0, pd.DataFrame()
            
            # åˆ†é›¢å·²å¡«å¯«å’Œæœªå¡«å¯«çš„è¨˜éŒ„
            filled_mask = (
                (df['äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ'].astype(str).str.strip() != '') &
                (df['äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ'].astype(str).str.strip().str.lower() != 'nan')
            )
            df_filled = df[filled_mask].copy()
            df_unfilled = df[~filled_mask].copy()
            
            if df_filled.empty:
                logger.warning(f"   âš ï¸ {file_path.name}: æ²’æœ‰å·²å¡«å¯«çš„è¨˜éŒ„")
                return pd.DataFrame(), 0, df_unfilled
            
            # å¾æª”åæå–ä¾›æ‡‰å•†åç¨±
            supplier_name = file_path.stem.split('éœ€è¦äººæ‰‹è™•ç†')[0] if 'éœ€è¦äººæ‰‹è™•ç†' in file_path.stem else ''
            
            # å°‡å¡«å¯«çš„è¨˜éŒ„åŠ å…¥ mapping
            mapping_count = 0
            processed_rows = []
            
            for idx, row in df_filled.iterrows():
                barcode = str(row['è²¨å“æ¢ç¢¼']).strip()
                product_name = str(row['è²¨å“åç¨±']).strip()
                product_code = str(row['äººæ‰‹è¼¸å…¥è²¨å“ç·¨è™Ÿ']).strip()
                
                if barcode and product_name and product_code:
                    mapping_manager.add_mapping(barcode, product_name, product_code, supplier_name)
                    mapping_count += 1
                    
                    # æº–å‚™è™•ç†å¾Œçš„è³‡æ–™
                    processed_row = {
                        'è²¨å“æ¢ç¢¼': barcode,
                        'è²¨å“åç¨±': product_name,
                        'å…¥è²¨åƒ¹': str(row.get('å…¥è²¨åƒ¹', '0')).strip(),
                        'å…¥è²¨é‡': str(row.get('å…¥è²¨é‡', '0')).strip(),
                        'è²¨å“ç·¨è™Ÿ': product_code,
                        'ä¾›æ‡‰å•†åç¨±': supplier_name,
                        'åº—è™Ÿ': 'S1',
                        'å…¥è²¨æ—¥æœŸ': datetime.now().strftime('%Y%m%d'),
                        'æ”¶æ“šå–®è™Ÿ': '',
                        'ä¾›æ‡‰å•†ç·¨è™Ÿ': '001',
                        'å‚™è¨»': '',
                        'ç‹€æ…‹': ''
                    }
                    processed_rows.append(processed_row)
            
            if processed_rows:
                processed_df = pd.DataFrame(processed_rows)
                # è½‰æ›æ•¸å€¼æ¬„ä½
                processed_df['å…¥è²¨åƒ¹'] = pd.to_numeric(processed_df['å…¥è²¨åƒ¹'], errors='coerce').fillna(0)
                processed_df['å…¥è²¨é‡'] = pd.to_numeric(processed_df['å…¥è²¨é‡'], errors='coerce').fillna(0).astype(int)
                processed_df = processed_df[processed_df['å…¥è²¨é‡'] > 0]
                
                logger.info(f"   âœ… å·²è™•ç† {len(processed_df)} ç­†ç”¢å“ï¼Œæ–°å¢ {mapping_count} ç­† mapping")
                if not df_unfilled.empty:
                    logger.info(f"   âš ï¸ é‚„æœ‰ {len(df_unfilled)} ç­†æœªå¡«å¯«çš„è¨˜éŒ„éœ€è¦ä¿ç•™")
                return processed_df, mapping_count, df_unfilled
            else:
                return pd.DataFrame(), 0, df_unfilled
                
        except Exception as e:
            logger.error(f"âŒ è™•ç†å¾…è™•ç†æª”å¤±æ•—: {e}")
            return pd.DataFrame(), 0, pd.DataFrame()

def main():

    base_dir = "workspace"
    # 1. è®€å–è¨­å®š
    config_mgr = ConfigManager(Path(base_dir))
    config_df = config_mgr.load_config()
    
    # å»ºç«‹ Mapping ç®¡ç†å™¨
    mapping_mgr = MappingManager(Path(base_dir))
    
    loader = BatchReceiptLoader(base_dir)
    cleaner = ReceiptCleaner(config_df)
    exporter = ReceiptExporter(base_dir)

    # è®€å–ä¸¦é©—è­‰åº«å­˜æ•¸æ“šæº
    input_stock = "data/processed/DetailGoodsStockToday.csv"
    if not os.path.exists(input_stock):
        logger.error("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ•¸æ“šæºã€‚")
        return
    
    # å»ºç«‹ç”¢å“é©—è­‰å™¨ï¼ˆå‚³å…¥ mapping_managerï¼‰
    validator = ProductValidator(input_stock, mapping_mgr)

    logger.info("ğŸš€ é–‹å§‹æ‰¹æ¬¡è™•ç†...")
    
    # æœå°‹é—œéµå­—ï¼šå¾è¨­å®šæª”è£¡çš„ 4 å€‹é—œéµæ¬„ä½æŠ“å­—
    search_keywords = []
    if not config_df.empty:
        target_cols = ["è²¨å“æ¢ç¢¼", "å…¥è²¨åƒ¹", "å…¥è²¨é‡", "è²¨å“åç¨±"]
        for col in target_cols:
            if col in config_df.columns:
                keywords = config_df[col].dropna().unique().tolist()
                search_keywords.extend([k for k in keywords if k.lower() != 'nan' and k.strip()])
    
    # è‹¥ Config æ²’æ±è¥¿ï¼Œçµ¦å€‹åŸºæœ¬é è¨­å€¼ä»¥å…ç¨‹å¼è·‘ä¸å‹•
    if not search_keywords:
        logger.warning("âš ï¸ Config ä¸­ç„¡é—œéµå­—ï¼Œè«‹è¨­å®šä¾›æ‡‰å•†è¨­å®šæª”ï¼")
        return

    # 2. å…ˆè™•ç†å¾…è™•ç†æª”æ¡ˆï¼ˆäººå·¥å¡«å¯«çš„ï¼‰
    logger.info("ğŸ“‹ æª¢æŸ¥å¾…è™•ç†æª”æ¡ˆ...")
    manual_files = [f for f in loader.get_pending_files() if 'éœ€è¦äººæ‰‹è™•ç†' in f.stem]
    for file_path in manual_files:
        logger.info(f"ğŸ“ è™•ç†å¾…è™•ç†æª”: {file_path.name}")
        processed_df, mapping_count, unfilled_df = exporter.process_manual_excel(file_path, mapping_mgr, validator, base_dir)
        
        if not processed_df.empty:
            # æœ‰å·²å¡«å¯«è¨˜éŒ„ â†’ éœ€è¦è™•ç†
            # åŒ¯å‡ºåˆ° POS æª”
            exporter.save_pos_excel(processed_df, file_path.name)
            logger.info(f"   âœ… å·²åŒ¯å‡º {len(processed_df)} ç­†ç”¢å“åˆ° POS åŒ¯å…¥æª”")
            
            # è™•ç†æœªå¡«å¯«çš„è¨˜éŒ„
            if not unfilled_df.empty:
                # æœ‰æœªå¡«å¯«çš„è¨˜éŒ„ï¼Œé‡æ–°ä¿å­˜
                supplier_name = file_path.stem.split('éœ€è¦äººæ‰‹è™•ç†')[0] if 'éœ€è¦äººæ‰‹è™•ç†' in file_path.stem else ''
                try:
                    exporter.save_unmatched_excel(unfilled_df, supplier_name, validator, base_dir)
                    logger.info(f"   ğŸ“‹ å·²æ›´æ–°å¾…è™•ç†æª”ï¼Œä¿ç•™ {len(unfilled_df)} ç­†æœªå¡«å¯«çš„è¨˜éŒ„")
                except Exception as e:
                    logger.error(f"   âŒ ä¿å­˜æœªå¡«å¯«è¨˜éŒ„å¤±æ•—: {e}")
                    logger.warning(f"   âš ï¸ ä¿ç•™åŸå§‹å¾…è™•ç†æª”ï¼Œæœªæ­¸æª”")
                    continue  # ä¿å­˜å¤±æ•—æ™‚ä¸æ­¸æª”ï¼Œé¿å…éºå¤±è³‡æ–™
            
            # ç„¡è«–æ˜¯å¦æœ‰æœªå¡«å¯«è¨˜éŒ„ï¼Œéƒ½æ­¸æª”åŸå§‹æª”ï¼ˆå› ç‚ºå·²ç¶“è™•ç†éäº†ï¼‰
            loader.archive_file(file_path)
            logger.info(f"   ğŸ“¦ åŸå§‹å¾…è™•ç†æª”å·²æ­¸æª”")
        else:
            # æ²’æœ‰å·²å¡«å¯«è¨˜éŒ„ â†’ ä¸ç”¨å‹•ï¼Œä¿ç•™åŸæª”æ¡ˆ
            logger.info(f"   â„¹ï¸ {file_path.name}: æ²’æœ‰å·²å¡«å¯«çš„è¨˜éŒ„ï¼Œä¿ç•™åŸæª”æ¡ˆç­‰å¾…è™•ç†")
            # ä¸æ­¸æª”ï¼Œä¿ç•™åœ¨ pending ä¸­
    
    # 3. è™•ç†æ”¶æ“šæª”æ¡ˆ
    logger.info("ğŸ“„ è™•ç†æ”¶æ“šæª”æ¡ˆ...")
    for file_path in loader.get_pending_files():
        # è·³éå¾…è™•ç†æª”æ¡ˆï¼ˆå·²ç¶“è™•ç†éäº†ï¼‰
        if 'éœ€è¦äººæ‰‹è™•ç†' in file_path.stem:
            continue
            
        raw_header_df, raw_data_df = loader.smart_load(file_path, search_keywords)
        
        if not raw_data_df.empty:
            clean_df, supplier_name = cleaner.process(raw_data_df)
            
            if clean_df is not None:
                # ç”¢å“é©—è­‰ï¼šåˆ†é›¢æœ‰å°æ‡‰å’Œæ‰¾ä¸åˆ°çš„ç”¢å“ï¼ˆæœƒè‡ªå‹•æª¢æŸ¥ mappingï¼‰
                matched_df, unmatched_df = validator.validate_products(clean_df, supplier_name)
                
                # è™•ç†æœ‰å°æ‡‰çš„ç”¢å“ï¼ˆæ­£å¸¸åŒ¯å‡º POS æª”ï¼‰
                if not matched_df.empty:
                    exporter.save_pos_excel(matched_df, file_path.name)
                    logger.info(f"   âœ… å·²åŒ¯å‡º {len(matched_df)} ç­†ç”¢å“åˆ° POS åŒ¯å…¥æª”")
                
                # è™•ç†æ‰¾ä¸åˆ°å°æ‡‰çš„ç”¢å“ï¼ˆå­˜å¾…è™•ç†æª”ï¼‰
                if not unmatched_df.empty:
                    exporter.save_unmatched_excel(unmatched_df, supplier_name, validator, base_dir)
                    logger.info(f"   âš ï¸ å·²æ¨™è¨˜ {len(unmatched_df)} ç­†ç”¢å“å¾…äººå·¥è™•ç†")
                
                # æ‰€æœ‰è™•ç†éçš„åŸå§‹æ”¶æ“šéƒ½æ­¸æª”åˆ° processed
                loader.archive_file(file_path)
                logger.info(f"   ğŸ“¦ åŸå§‹æ”¶æ“šå·²æ­¸æª”")
            else:
                logger.error("   âŒ æ¸…æ´—å¤±æ•— (æœªè­˜åˆ¥æˆ–æ ¼å¼éŒ¯èª¤)")
        
    logger.info("ç¨‹å¼åŸ·è¡Œå®Œæˆï¼Œç­‰å¾…ç”¨æˆ¶ç¢ºèª...")
    logger.info("-" * 30)


# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    main()



