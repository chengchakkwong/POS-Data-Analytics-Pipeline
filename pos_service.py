import os
import time
import pandas as pd
from datetime import date, datetime, timedelta

class POSDataService:
    """
    ã€æ¥­å‹™é‚è¼¯å±¤ã€‘
    å°ˆé–€è² è²¬è™•ç† POS çš„æ•¸æ“šé‚è¼¯ï¼ˆåº«å­˜ã€éŠ·å”®ã€çµæ§‹ï¼‰ã€‚
    åªå°ˆæ³¨æ–¼æ•¸æ“šå¦‚ä½•æå–èˆ‡åŠ å·¥ã€‚
    """
    def __init__(self, db_manager):
        self.db = db_manager
        self.today = date.today().strftime('%Y-%m-%d')
        
    def generate_data_dictionary(self, output_file="table_structure.txt"):
        print("ğŸ” æ­£åœ¨ç”Ÿæˆæ•¸æ“šå­—å…¸...")
        sql = """
            SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH, IS_NULLABLE
            FROM INFORMATION_SCHEMA.COLUMNS
            ORDER BY TABLE_NAME, ORDINAL_POSITION
        """
        df = self.db.execute_query(sql)
        if df.empty: return

        with open(output_file, "w", encoding="utf-8") as f:
            current_table = ""
            for _, row in df.iterrows():
                if row['TABLE_NAME'] != current_table:
                    current_table = row['TABLE_NAME']
                    f.write(f"\nğŸ“¦ dbo.{current_table}\n")
                
                max_len = f"({int(row['CHARACTER_MAXIMUM_LENGTH'])})" if pd.notnull(row['CHARACTER_MAXIMUM_LENGTH']) and row['CHARACTER_MAXIMUM_LENGTH'] > 0 else ""
                f.write(f"   â”œâ”€ {row['COLUMN_NAME']:<30} {row['DATA_TYPE']:<12} {max_len:<8} NULL={row['IS_NULLABLE']}\n")
        print(f"âœ… çµæ§‹å·²å­˜è‡³ {output_file}")
 
    def get_stock_master_data(self):
        """
        æå–å®Œæ•´çš„å•†å“åº«å­˜èˆ‡åˆ†é¡è³‡è¨Šã€‚
        åœ¨æ­¤éšæ®µå³é€²è¡Œæ•¸æ“šæ¸…æ´—ï¼Œç¢ºä¿ç”¢å‡ºçš„ CSV æ˜¯æ•´æ½”ä¸”å¯ç›´æ¥ä½¿ç”¨çš„ã€‚
        """
        print("ğŸš€ æ­£åœ¨åŸ·è¡Œå…¨é‡å•†å“åº«å­˜é—œè¯æŸ¥è©¢...")
        
        sql = """
            SELECT 
                g.SID AS GoodsID, 
                g.ID AS ProductCode, 
                g.Barcode, 
                g.Name, 
                g.Note, 
                s.CurrStock, 
                g.RetailPrice, 
                g.LastInCost, 
                g.AvgCost,
                d.Name AS Category,          
                t1.Name AS InboundLocation,  
                t2.Name AS Supplier          
            FROM dbo.GoodsInfo g
            LEFT JOIN dbo.GoodsStock s ON g.SID = s.GoodsID AND s.ShopID = 1
            LEFT JOIN dbo.Dept d ON g.DeptID = d.SID
            LEFT JOIN dbo.ProductType1 t1 ON g.ProductType1ID = t1.SID
            LEFT JOIN dbo.ProductType2 t2 ON g.ProductType2ID = t2.SID
        """
        df = self.db.execute_query(sql)
        
        if not df.empty:
            # --- åœ¨æºé ­æ¸…æ´—æ•¸æ“š (Clean at Source) ---
            # 1. æ¸…æ´—ç”¢å“åç¨±èˆ‡å‚™è¨»ä¸­çš„æ›è¡Œç¬¦èˆ‡å‰å¾Œç©ºç™½
            # é€™æ¨£ç”¢å‡ºçš„ CSV å°±ä¸æœƒå†æœ‰è¡Œæ•¸éŒ¯äº‚çš„å•é¡Œ
            text_columns = ['Name', 'Note', 'Category', 'InboundLocation', 'Supplier']
            for col in text_columns:
                if col in df.columns:
                    df[col] = df[col].astype(str).str.replace(r'[\n\r\t]+', ' ', regex=True).str.strip()

            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            if not os.path.exists('data/processed'):
                os.makedirs('data/processed')
            
            output_file = "data/processed/DetailGoodsStockToday.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"âœ… ä»Šæ—¥å®Œæ•´åº«å­˜æ¸…å–®å·²æ›´æ–°: {output_file}")
            
        return df
    
    def sync_daily_sales(self, cache_file="data/processed/vw_GoodsDailySales_cache.parquet"):
        """
        å¢é‡åŒæ­¥æ¯æ—¥éŠ·å”®æ•¸æ“šã€‚
        å»ºè­°å°‡å¿«å–æª”æ¡ˆå­˜æ”¾åœ¨ data/processed/ æ–‡ä»¶å¤¾ä¸­ã€‚
        """
        start_time = time.perf_counter()

        # åˆ¤æ–·å¿«å–æ˜¯å¦å­˜åœ¨
        if os.path.exists(cache_file):
            df_old = pd.read_parquet(cache_file)
            last_date = pd.to_datetime(df_old["rDate"].max())
            # å¦‚æœæœ‰å¿«å–ï¼Œå¾€å‰æ¨ä¸€å¤©é€²è¡Œå¢é‡åŒæ­¥ï¼ˆç¢ºä¿æœ€å¾Œä¸€å¤©çš„æ•¸æ“šå®Œæ•´æ€§ï¼‰
            sync_start = (last_date - timedelta(days=1)).strftime("%Y-%m-%d")
            print(f"ğŸ“… ç™¼ç¾ç¾æœ‰å¿«å–ï¼Œæœ€å¾Œæ—¥æœŸç‚º {last_date.date()}ï¼Œå¾ {sync_start} é–‹å§‹å¢é‡åŒæ­¥...")
        else:
            df_old = pd.DataFrame()
            # è‹¥ç„¡å¿«å–ï¼Œç›´æ¥è¨­å®šç‚º 2024-01-01
            sync_start = "2024-01-01"
            print(f"â„¹ï¸ ç„¡ç¾æœ‰å¿«å–ï¼Œå°‡å¾åˆå§‹è¨­å®šæ—¥æœŸ {sync_start} é–‹å§‹å…¨é‡åŒæ­¥...")

        # é€™è£¡åªä½¿ç”¨ sync_startï¼Œä¸è¦å†é‡æ–°æŒ‡å®š
        print(f"ğŸ”„ æ­£åœ¨å¢é‡åŒæ­¥è‡ª {sync_start} èµ·çš„éŠ·å”®æ•¸æ“š...")


        sql = """
            SELECT D.GoodsID, M.rDate, SUM(D.Quantity) AS TotalQty, SUM(D.FinalAmt) AS TotalAmt
            FROM dbo.SalesDetail AS D
            JOIN dbo.SalesMaster AS M ON D.SalesMasterID = M.SID
            WHERE CONVERT(date, CONVERT(varchar(8), M.rDate)) >= :start_date
            GROUP BY D.GoodsID, M.rDate
        """
        df_new = self.db.execute_query(sql, params={"start_date": sync_start})

        if df_new.empty:
            print("âœ… ç„¡æ–°æ•¸æ“šã€‚")
            return df_old
        
        # åˆä½µæ–°èˆŠæ•¸æ“šä¸¦å»é‡ï¼ˆä»¥ GoodsID å’Œ rDate ç‚ºæº–ï¼Œä¿ç•™æœ€æ–°çš„ç´€éŒ„ï¼‰
        df_all = pd.concat([df_old, df_new], ignore_index=True).drop_duplicates(subset=["GoodsID", "rDate"], keep="last")
        
        # å„²å­˜å¿«å– 
        try:
            df_all.to_parquet(cache_file, compression="snappy")
        except ImportError:
            print("âŒ å„²å­˜å¤±æ•—ï¼šç’°å¢ƒä¸­ç¼ºå°‘ Parquet å¼•æ“ (pyarrow)ã€‚")
            print("ğŸ’¡ è«‹åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼špip install pyarrow")
        except Exception as e:
            # å¦‚æœæ˜¯ Snappy ä¸æ”¯æ´ï¼Œå˜—è©¦ gzip
            print(f"âš ï¸ Snappy å£“ç¸®å¤±æ•—ï¼Œå˜—è©¦ gzip: {e}")
            df_all.to_parquet(cache_file, compression="gzip")
        
        duration = time.perf_counter() - start_time
        print(f"ğŸ’¾ åŒæ­¥å®Œæˆï¼å…± {len(df_all):,} ç­†ï¼Œè€—æ™‚ {duration:.2f} ç§’")
        return df_all